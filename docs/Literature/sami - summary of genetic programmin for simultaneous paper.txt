Genetic Programming for Simultaneous Feature Selection and Classifier Design

Durga Prasad Muni, Nikhil R. Pal, Fellow, IEEE, and Jyotirmoy Das


- two new crossover operations to suit feature selection process
- algorithm produces a feature ranking scheme
- evaluation function that is used: distance, information, dependence, consistency, classifier error rate (note: these 5 groups all have references that can be found in this paper)
- multitree genetic programming based FS uses a population of classifiers. Each of these classifiers is created using a randomly chosen subset of the features - it is more probable that a smaller subset is created. More accurate classifiers are more likely to be selected.
- because of GP, the classifier is provided as a mathematical representation
-two-class problem: represent as binary tree

- select a classifier based on fitness for:
- reproduction (FPS)
- crossover (tournament selection)
- mutation (random selection)

- trees that are unable to properly classify are more likely to take part in crossover and mutation

- classifier has an option of “I don’t know” if all trees have negative response
- if one tree shows positive response, X is assigned to the class associated with that tree
- if more than 1 tree shows a positive response, resolve conflict with weight based scheme

- fitness function: “higher fitness value to classifier which can classify correctly more samples using fewer features” - multi objective 

- 2 crossover operations:
1) homogeneous crossover (increases through generations) - restricts crossover between parents which use the same feature subset
2) heterogeneous crossover (depends on degree of similarity between two parents) (decreases through generations) - allows crossover between classifiers using different feature subsets

- one of their data sets was the Wisconsin breast cancer data set  - ends up only 3 features were important
- for this, they removed anything with missing data
- another data set was the Wisconsin Diagnostic Breast Cancer data set
- for all their data, they used pop sizes from range 1000-5000 (usually from 1000-2000) - ends up only 6 or 7 features were important
- won’t write them all here, but Table II includes all their parameters, which could be super useful as a baseline for us

- the pseudocode algorithms are written on page 10

- Sami’s thoughts: it could be interesting to use a similar fitness function as this paper, because it lets them change the weight of how much is penalized for using too many features. if we have a larger weight, maybe we could figure out the most important features. we could slowly decrease the weight, and therefore reveal one-at-a-time which features are also important.